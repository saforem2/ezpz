[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ezpz ğŸ‹",
    "section": "",
    "text": "ezpz ğŸ‹\n\n\n\n\n\nLaunch and train across all your accelerators, using your favorite framework + backend combo.\nezpz simplifies the process of:\n\n\n\nSetting up + launching distributed training:\n\n\n\n\nimport ezpz as ez\n\n\nRANK = ez.setup_torch(backend=backend) for backend \\in {DDP, deepspeed, horovod}\nRANK = ez.get_rank()\nLOCAL_RANK = ez.get_local_rank()\nWORLD_SIZE = ez.get_world_size()\n\n(see ezpz/dist.py for more details).\n\n\n\n\n\n\n\nUsing your favorite framework:\n\n\nframework=pytorch + backend={DDP, deepspeed, horovod}\nframework=tensorflow + backend=horovod\nez.get_torch_device(): {cuda, xpu, mps, cpu}\nez.get_torch_backend(): {nccl, ccl, gloo}\n\n2ez ğŸ˜. (see frameworks for additional details)\n\n\n\n\n\nWriting device agnostic code:\n\n\n\n\nezpz.get_torch_device()\n\n&gt;&gt;&gt; import ezpz as ez\n&gt;&gt;&gt; DEVICE = ez.get_torch_device()\n&gt;&gt;&gt; model = torch.nn.Linear(10, 10)\n&gt;&gt;&gt; model.to(DEVICE)\n&gt;&gt;&gt; x = torch.randn((10, 10), device=DEVICE)\n&gt;&gt;&gt; y = model(x)\n&gt;&gt;&gt; y.device\ndevice(type='mps', index=0)\n\n\n\n\n\n\n\nUsing wandb:\n\n\nez.setup_wandb(project_name='ezpz')\n\n\n\n\nFull support for any {device + framework + backend}:\n\ndevice: {GPU, XPU, MPS, CPU}\nframework: {torch, deepspeed, horovod, tensorflow}\nbackend: {DDP, deepspeed, horovod}"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "ezpz ğŸ‹",
    "section": "",
    "text": "ezpz ğŸ‹\n\n\n\n\n\nLaunch and train across all your accelerators, using your favorite framework + backend combo.\nezpz simplifies the process of:\n\n\n\nSetting up + launching distributed training:\n\n\n\n\nimport ezpz as ez\n\n\nRANK = ez.setup_torch(backend=backend) for backend \\in {DDP, deepspeed, horovod}\nRANK = ez.get_rank()\nLOCAL_RANK = ez.get_local_rank()\nWORLD_SIZE = ez.get_world_size()\n\n(see ezpz/dist.py for more details).\n\n\n\n\n\n\n\nUsing your favorite framework:\n\n\nframework=pytorch + backend={DDP, deepspeed, horovod}\nframework=tensorflow + backend=horovod\nez.get_torch_device(): {cuda, xpu, mps, cpu}\nez.get_torch_backend(): {nccl, ccl, gloo}\n\n2ez ğŸ˜. (see frameworks for additional details)\n\n\n\n\n\nWriting device agnostic code:\n\n\n\n\nezpz.get_torch_device()\n\n&gt;&gt;&gt; import ezpz as ez\n&gt;&gt;&gt; DEVICE = ez.get_torch_device()\n&gt;&gt;&gt; model = torch.nn.Linear(10, 10)\n&gt;&gt;&gt; model.to(DEVICE)\n&gt;&gt;&gt; x = torch.randn((10, 10), device=DEVICE)\n&gt;&gt;&gt; y = model(x)\n&gt;&gt;&gt; y.device\ndevice(type='mps', index=0)\n\n\n\n\n\n\n\nUsing wandb:\n\n\nez.setup_wandb(project_name='ezpz')\n\n\n\n\nFull support for any {device + framework + backend}:\n\ndevice: {GPU, XPU, MPS, CPU}\nframework: {torch, deepspeed, horovod, tensorflow}\nbackend: {DDP, deepspeed, horovod}"
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "ezpz ğŸ‹",
    "section": "ğŸ“ Example",
    "text": "ğŸ“ Example\nWe provide below a complete example that will launch test_dist.py (included below) across all GPUs in your current {PBS, slurm} job and train a simple model using either DDP or deepspeed\n\n\ntest_dist.py\n\n\n\"\"\"\nezpz_ddp.py\n\n- to launch:\n\n$ source ezpz/src/ezpz/bin/savejobenv\n$ BACKEND=DDP launch python3 ezpz_ddp.py\n\"\"\"\nimport os\nimport logging\nimport time\nfrom typing import Optional\nimport torch\nimport ezpz as ez\n\n# backend can be any of DDP, deespepeed, horovod\nRANK = ez.setup_torch(\n  backend=(\n      backend := os.environ.get('BACKEND', 'DDP')\n  ),\n  port=(\n      port := os.environ.get(\"MASTER_PORT\", \"29500\")\n  )\n)\n# RANK = DIST_INIT['rank']\n# WORLD_SIZE = DIST_INIT['world_size']\n# LOCAL_RANK = DIST_INIT['local_rank']\n# if DEVICE == \"cuda\" and torch.cuda.is_available():\n#     torch.cuda.set_device(LOCAL_RANK)\nDEVICE = ez.get_torch_device()\nWORLD_SIZE = ez.get_world_size()\nLOCAL_RANK = ez.get_local_rank()\nDEVICE_ID = f\"{DEVICE}:{LOCAL_RANK}\"\n\n\n# log only from RANK == 0\nlogger = logging.getLogger(__name__)\nlogger.setLevel(\"INFO\") if RANK == 0 else logger.setLevel(\"CRITICAL\")\n\nBATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 64))  # 64\nINPUT_SIZE = int(os.environ.get(\"INPUT_SIZE\", 128))  # 128\nOUTPUT_SIZE = int(os.environ.get(\"OUTPUT_SIZE\", 128))  # 128\nDTYPE = os.environ.get(\"DTYPE\", torch.get_default_dtype())\nTRAIN_ITERS = int(os.environ.get(\"TRAIN_ITERS\", 50))\n\n# logger.info(f\"{DIST_INIT=}\")\n\n\nclass Network(torch.nn.Module):\n  def __init__(\n          self,\n          input_dim: int = 128,\n          output_dim: int = 128,\n          sizes: Optional[list[int]] = None,\n  ):\n      super(Network, self).__init__()\n      if sizes is None:\n          self.layers = torch.nn.Linear(input_dim, output_dim)\n      elif len(sizes) &gt; 0:\n          layers = [torch.nn.Linear(input_dim, sizes[0])]\n          for idx, size in enumerate(sizes[1:]):\n              layers.append(\n                  torch.nn.Linear(sizes[idx], size)\n              )\n          layers.append(torch.nn.Linear(sizes[-1], output_dim))\n          self.layers = torch.nn.Sequential(*layers)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n      return self.layers(x)\n\n\ndef calc_loss(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n  return (y - x).pow(2).sum()\n\n\ndef plot_losses(losses: dict) -&gt; None:\n  import plotext as pltx\n  # y = list(losses.values())\n  pltx.theme('clear')\n  pltx.scatter(list(losses.values()))\n  pltx.show()\n  pltx.save_fig(\"test_dist_losses.txt\")\n  pltx.ylabel(\"loss\")\n  pltx.xlabel(\"iteration\")\n\n\ndef main():\n  model = Network(\n      input_dim=INPUT_SIZE,\n      output_dim=OUTPUT_SIZE,\n      sizes=[1024, 512, 256, 128]\n  )\n  model.to(DEVICE)\n  model.to(DEVICE_ID)\n  logger.info(f'{model=}')\n  optimizer = torch.optim.Adam(model.parameters())\n  if backend.lower() == 'ddp':\n      if WORLD_SIZE &gt; 1:\n          from torch.nn.parallel import DistributedDataParallel as DDP\n          model = DDP(\n              model,\n              device_ids=[]\n          )\n  elif backend.lower() in ('ds', 'deepspeed'):\n      import deepspeed\n      # config = ez.load_ds_config().update(\n      #     {\"train_micro_batch_size_per_gpu\": BATCH_SIZE}\n      # )\n      import argparse\n      parser = argparse.ArgumentParser(\n          description='My training script.'\n      )\n      parser.add_argument(\n          '--local_rank',\n          required=False,\n          type=int,\n          default=-1,\n          # default=ez.get_local_rank()),\n          help='local rank passed from distributed launcher',\n      )\n      # Include DeepSpeed configuration arguments\n      parser = deepspeed.add_config_arguments(parser)\n      cmd_args = parser.parse_args()\n      logger.info(f'{cmd_args=}')\n      model, optimizer, *_ = deepspeed.initialize(\n          args=cmd_args,\n          model=model,\n          optimizer=optimizer,\n      )\n\n  losses = {}\n  for iter in range(TRAIN_ITERS):\n      t0 = time.perf_counter()\n      x = torch.rand((BATCH_SIZE, INPUT_SIZE), dtype=DTYPE).to(DEVICE)\n      y = model(x)\n      loss = calc_loss(x, y)\n      losses[iter] = loss\n      dtf = ((t1 := time.perf_counter()) - t0)\n      if backend == 'deepspeed':\n          model.backward(loss)\n          model.step(loss)\n      else:\n          loss.backward()\n          optimizer.step()\n      optimizer.zero_grad()\n      dtb = time.perf_counter() - t1\n      logger.info(\n          ', '.join([\n              f'{iter=}',\n              f'loss={loss.item():.5f}',\n              f'dt={dtf+dtb:.3f}',\n              f'{dtf=:.3f}',\n              f'{dtb=:.3f}'\n          ])\n      )\n  if RANK == 0:\n      plot_losses(losses)\n\n\nif __name__ == '__main__':\n  main()\n\n\n\nğŸƒğŸ»â€â™‚ï¸ Running\n\ngit clone + pip install ezpz:\n$ git clone https://github.com/saforem2/ezpz\n$ python3 -m pip install -e ezpz\n[optional] If using PBS or slurm:\n\n\n\nSave Job info:\n\n\nsavejobenv:\n$ source ezpz/src/ezpz/bin/savejobenv\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ Writing PBS vars to /home/foremans/.pbsenv\nâ”‚ HOSTFILE: /var/spool/pbs/aux/8992614.amn-0001\nâ”‚ NHOSTS: 2\nâ”‚ NGPU_PER_HOST: 12 GPUs per host\nâ”‚ NGPUS: 24 GPUs total\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [DIST INFO]:\nâ”‚   â€¢ Writing Job info to /home/foremans/.pbsenv\nâ”‚     â€¢ HOSTFILE: /var/spool/pbs/aux/8992614.amn-0001\nâ”‚     â€¢ NHOSTS: 2\nâ”‚     â€¢ NGPU_PER_HOST: 12\nâ”‚     â€¢ NGPUS = (NHOSTS * NGPU_PER_HOST) = 24\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Hosts]:\nâ”‚       â€¢ x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com, x1921c0s2b0n0.hostmgmt2000.cm.americas.sgi.com\nâ”‚     â€¢ [host:0] - x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\nâ”‚     â€¢ [host:1] - x1921c0s2b0n0.hostmgmt2000.cm.americas.sgi.com\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ YOU ARE HERE: /home/foremans\nâ”‚ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Launch]:\nâ”‚     â€¢ Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/8992614.amn-0001)\nâ”‚       to launch job\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nthis will automatically define a launch alias:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Launch]:\nâ”‚     â€¢ Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/8992614.amn-0001)\nâ”‚       to launch job\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\nLaunch test_dist.py:\n\nDDP:\n$ launch python3 -m ezpz.test_dist\nDeepSpeed:\n$ BACKEND=deepspeed launch python3 -m ezpz.test_dist --deepspeed --deepspeed_config ezpz/src/ezpz/conf/ds_config.json\nOutput:\n\n\n\nGPU\n\n$ launch python3 -m ezpz.test_dist |& tee ezpz-test-dist.log\n\nConnected to tcp://x3005c0s13b0n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /lus/eagle/projects/datascience/foremans/miniconda3/envs/2024-04-20/bin/python3\nLaunching application 9e4c8311-1729-4385-b1d2-d4cd6006ac1d\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=1/7][local_rank=1/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=5/7][local_rank=1/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=3/7][local_rank=3/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=7/7][local_rank=3/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=4/7][local_rank=0/3][node=0/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=6/7][local_rank=2/3][node=0/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=2/7][local_rank=2/3][node=0/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=0/7][local_rank=0/3][node=0/1]\n[2024-04-20 19:26:22][WARNING][dist:296] - Using [8 / 8] available \"cuda\" devices !!\n[2024-04-20 19:26:22][INFO][test_dist:46] - DIST_INIT={'world_size': 8, 'rank': 0, 'local_rank': 0}\n[2024-04-20 19:26:24][INFO][test_dist:84] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=0, loss=2789.99072, dt=0.664, dtf=0.659, dtb=0.005\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=1, loss=1961.33459, dt=0.002, dtf=0.001, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=2, loss=1450.47461, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=3, loss=1088.81958, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=4, loss=945.28839, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=5, loss=906.78857, dt=0.002, dtf=0.000, dtb=0.001\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=6, loss=789.18243, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=7, loss=751.63477, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=8, loss=735.62915, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=9, loss=732.12775, dt=0.002, dtf=0.000, dtb=0.001\n\n\n\nXPU\n\n# [04:50:57 PM] [foremans@x1921c0s0b0n0] ~/q/llm.devkit/Megatron-DeepSpeed/dep/ezpz/s/ezpz ï˜ main q4-drop 32s\n$ launch python3 -Wignore test_dist.py\nConnected to tcp://x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com:7919\nFound executable /home/foremans/miniconda3/envs/q4-drop/bin/python3\nLaunching application 5bf3e9e8-89fb-412a-a49e-3c81601436b7\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=9/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=14/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=3/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=17/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=6/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=13/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=7/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=19/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=8/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=21/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=10/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=22/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=11/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=23/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=2/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=20/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=4/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=15/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=18/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=12/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=1/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=16/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=5/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"ccl\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/8992337.amn-0001\",\n    \"HOSTNAME\": \"x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\",\n    \"HOSTS\": \"['x1921c0s0b0n0', 'x1921c0s5b0n0']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"SunSpot\",\n    \"NGPUS\": 24,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 24,\n    \"WORLD_SIZE_TOTAL\": 24\n}\n[2024-04-19 16:51:06][INFO][dist:602] - Using oneccl_bindings from: /lus/gila/projects/Aurora_deployment/foremans/q4-drop_sunspot/llm.devkit/torch-ccl/oneccl_bindings_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:604] - Using ipex from: /home/foremans/miniconda3/envs/q4-drop/lib/python3.9/site-packages/intel_extension_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:605] - [0/24] Using device='xpu' with backend='DDP' + 'ccl' for distributed training.\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=0/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][WARNING][dist:296] - Using [24 / 24] available \"xpu\" devices !!\n2024:04:19-16:51:06:(16909) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n[2024-04-19 16:51:06][INFO][test_dist:71] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=0, loss=2709.53418, dt=1.380, dtf=0.950, dtb=0.430\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=1, loss=2058.49805, dt=0.133, dtf=0.002, dtb=0.131\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=2, loss=1507.91187, dt=0.004, dtf=0.001, dtb=0.004\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=3, loss=1181.78577, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=4, loss=949.43561, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=5, loss=848.14905, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=6, loss=788.76123, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=7, loss=753.59509, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=8, loss=750.62225, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=9, loss=740.23474, dt=0.004, dtf=0.001, dtb=0.003\nApplication 5bf3e9e8 resources: utime=621s stime=111s maxrss=1746816KB inblock=192 oublock=16 minflt=10719359 majflt=7493 nvcsw=169332 nivcsw=77546\n\n\n\n\n\nCPU\n\n$ TORCH_DEVICE=cpu mpirun -np 12 python3 test_dist.py\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=1/11][local_rank=1/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=3/11][local_rank=3/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=6/11][local_rank=6/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=5/11][local_rank=5/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=2/11][local_rank=2/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=10/11][local_rank=10/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=4/11][local_rank=4/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=7/11][local_rank=7/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=9/11][local_rank=9/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=11/11][local_rank=11/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=8/11][local_rank=8/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"cpu\",\n    \"DEVICE_ID\": \"cpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/Users/samforeman/projects/saforem2/ezpz/src/ezpz/hostfile\",\n    \"HOSTNAME\": \"Sams-MacBook-Pro.local\",\n    \"HOSTS\": \"['Sams-MacBook-Pro']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Sams-MacBook-Pro.local\",\n    \"NGPUS\": 12,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 1,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"LOCAL\",\n    \"WORLD_SIZE_IN_USE\": 12,\n    \"WORLD_SIZE_TOTAL\": 12\n}\n[2024-04-19 14:44:13][INFO][dist:605] - [0/12] Using device='cpu' with backend='DDP' + 'gloo' for distributed training.\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=0/11][local_rank=0/11][node=0/0]\n[2024-04-19 14:44:13][WARNING][dist:296] - Using [12 / 12] available \"cpu\" devices !!\n[2024-04-19 14:44:13][INFO][test_dist:72] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=0, loss=2801.62549, dt=0.389, dtf=0.042, dtb=0.348\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=1, loss=2092.84692, dt=0.051, dtf=0.010, dtb=0.041\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=2, loss=1482.45520, dt=0.037, dtf=0.004, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=3, loss=1174.38037, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=4, loss=938.39917, dt=0.032, dtf=0.003, dtb=0.030\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=5, loss=888.37390, dt=0.035, dtf=0.001, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=6, loss=784.63470, dt=0.036, dtf=0.003, dtb=0.032\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=7, loss=749.53839, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=8, loss=732.22656, dt=0.036, dtf=0.003, dtb=0.034\n[2024-04-19 14:44:15][INFO][test_dist:102] - iter=9, loss=730.63776, dt=0.034, dtf=0.001, dtb=0.033\n35.68s user 17.20s system 546% cpu 9.681s total"
  },
  {
    "objectID": "index.html#helper-utilities",
    "href": "index.html#helper-utilities",
    "title": "ezpz ğŸ‹",
    "section": "ğŸ§° Helper Utilities",
    "text": "ğŸ§° Helper Utilities\nWe provide some shell scripts that are useful when working with a job scheduler (e.g.Â PBS Pro @ ALCF or slurm elsewhere).\n\nsrc/ezpz/bin/savejobenv:\nShell script to save relevant job related environment variables to a file which can be sourced from new login instances.\n\n\n\nsavejobenv\n\n\nLaunch a job, clone (or navigate into) ezpz, and source src/ezpz/bin/savejobenv:\n\n(thetalogin4) $ qsub-gpu -A datascience -n 2 -q full-node --attrs=\"filesystems=home,grand,eagle,theta-fs0:ssds=required\" -t 06:00 -I\nJob routed to queue \"full-node\".\nWait for job 10155652 to start...\nOpening interactive session to thetagpu04\n[...]\n(thetagpu04) $ git clone https://github.com/saforem2/ezpz\n(thetagpu04) $ source ezpz/src/ezpz/bin/savejobenv\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ Writing COBALT vars to /home/foremans/.cobaltenv\nâ”‚ HOSTFILE: /var/tmp/cobalt.10155652\nâ”‚ NHOSTS: 2\nâ”‚ 8 GPUs per host\nâ”‚ 16 GPUs total\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [DIST INFO]:\nâ”‚   â€¢ Writing Job info to /home/foremans/.cobaltenv\nâ”‚     â€¢ HOSTFILE: /var/tmp/cobalt.10155652\nâ”‚     â€¢ NHOSTS: 2\nâ”‚     â€¢ NGPU_PER_HOST: 8\nâ”‚     â€¢ NGPUS = (NHOSTS * NGPU_PER_HOST) = 16\nâ”‚ [Hosts]:\nâ”‚       â€¢ thetagpu04 thetagpu19\nâ”‚ [Launch]:\nâ”‚     â€¢ Use: 'launch' (=mpirun -n  -N  --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH)\nâ”‚       to launch job\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ YOU ARE HERE: /home/foremans\nâ”‚ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nsrc/ezpz/bin/getjobenv:\nShell script that, when sourced, will populate the current environment with the necessary job-related variables.\n\n\n\ngetjobenv\n\n\nNow, in a NEW SHELL\n(localhost)   $ ssh &lt;user&gt;@theta\n(thetalogin4) $ ssh thetagpu19\n(thetagpu19)  $ module load conda/2023-01-11; conda activate base\n(thetagpu19)  $ cd ezpz\n(thetagpu19)  $ source ./src/ezpz/bin/getjobenv\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Hosts]: \nâ”‚     â€¢ thetagpu04, thetagpu19\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [DIST INFO]: \nâ”‚     â€¢ Loading job env from: /home/foremans/.cobaltenv\nâ”‚     â€¢ HOSTFILE: /var/tmp/cobalt.10155652\nâ”‚     â€¢ NHOSTS: 2\nâ”‚     â€¢ NGPU_PER_HOST: 8\nâ”‚     â€¢ NGPUS (NHOSTS x NGPU_PER_HOST): 16\nâ”‚     â€¢ DIST_LAUNCH: mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\nâ”‚     â€¢ Defining alias: launch: aliased to mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n(thetagpu19) $ mkdir -p venvs/thetaGPU/2023-01-11\n(thetagpu19) $ python3 -m venv venvs/thetaGPU/2023-01-11 --system-site-packages\n(thetagpu19) $ source venvs/thetaGPU/2023-01-11/bin/activate\n(thetagpu19) $ python3 -m pip install -e . --require-virtualenv\n(thetagpu19) $ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-10-26 12:21:26,716][ezpz.dist][INFO] - Using DDP for distributed training\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 13\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 14\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 8\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 12\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 9\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 10\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 15\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 11\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2\n[2023-10-26 12:21:26,798][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n[2023-10-26 12:21:26,811][torch.distributed.distributed_c10d][INFO] - Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,812][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,814][torch.distributed.distributed_c10d][INFO] - Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,815][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,816][torch.distributed.distributed_c10d][INFO] - Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,817][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,819][torch.distributed.distributed_c10d][INFO] - Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,820][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,821][torch.distributed.distributed_c10d][INFO] - Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,823][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,827][torch.distributed.distributed_c10d][INFO] - Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,828][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,830][torch.distributed.distributed_c10d][INFO] - Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,831][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:27,035][ezpz.dist][INFO] - RANK: 0 / 15\n{\n  \"framework\": \"pytorch\",\n  \"backend\": \"DDP\",\n  \"use_wandb\": false,\n  \"seed\": null,\n  \"port\": null,\n  \"ds_config_path\": null,\n  \"wandb_project_name\": null,\n  \"precision\": null,\n  \"ngpus\": null\n}\n[2023-10-26 12:21:27,038][__main__][INFO] - Output dir: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/ezpz/outputs/runs/pytorch/DDP/2023-10-26/12-21-25\n[2023-10-26 12:21:27,097][ezpz.dist][INFO] - RANK: 8 / 15\n[2023-10-26 12:21:27,103][ezpz.dist][INFO] - RANK: 6 / 15\n[2023-10-26 12:21:27,104][ezpz.dist][INFO] - RANK: 14 / 15\n[2023-10-26 12:21:27,111][ezpz.dist][INFO] - RANK: 13 / 15\n[2023-10-26 12:21:27,116][ezpz.dist][INFO] - RANK: 1 / 15\n[2023-10-26 12:21:27,126][ezpz.dist][INFO] - RANK: 7 / 15\n[2023-10-26 12:21:27,135][ezpz.dist][INFO] - RANK: 10 / 15\n[2023-10-26 12:21:27,139][ezpz.dist][INFO] - RANK: 12 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 9 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 15 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 11 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 5 / 15\n[2023-10-26 12:21:27,144][ezpz.dist][INFO] - RANK: 2 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 4 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 3 / 15\n16.56s user 30.05s system 706% cpu 6.595s total\nwhile this example looked at ThetaGPU, the exact same process will work on any of {ThetaGPU, Polaris, Perlmutter}.\n\n\n\n\n\n\n\n\n\n\nâ¤ï¸â€ğŸ©¹ Status\n\n\n\n\n\n\n\nLast Updated: 05/13/2024 @ 19:16:46"
  },
  {
    "objectID": "slides.html#overview",
    "href": "slides.html#overview",
    "title": "ezpz ğŸ‹",
    "section": "ğŸ‘€ Overview",
    "text": "ğŸ‘€ Overview\n\n\n\n\nezpz ğŸ‹\n\n\nLaunch and train across all your accelerators, using your favorite framework + backend combo.\nezpz simplifies the process of:\n\n\n\nSetting up + launching distributed training:\n\n\n\n\nimport ezpz as ez\n\n\nRANK = ez.setup_torch(backend=backend) for backend \\in {DDP, deepspeed, horovod}\nRANK = ez.get_rank()\nLOCAL_RANK = ez.get_local_rank()\nWORLD_SIZE = ez.get_world_size()\n\n(see ezpz/dist.py for more details).\n\n\n\n\n\n\n\nUsing your favorite framework:\n\n\nframework=pytorch + backend={DDP, deepspeed, horovod}\nframework=tensorflow + backend=horovod\nez.get_torch_device(): {cuda, xpu, mps, cpu}\nez.get_torch_backend(): {nccl, ccl, gloo}\n\n2ez ğŸ˜. (see frameworks for additional details)\n\n\n\n\n\nWriting device agnostic code:\n\n\n\n\nezpz.get_torch_device()\n\n&gt;&gt;&gt; import ezpz as ez\n&gt;&gt;&gt; DEVICE = ez.get_torch_device()\n&gt;&gt;&gt; model = torch.nn.Linear(10, 10)\n&gt;&gt;&gt; model.to(DEVICE)\n&gt;&gt;&gt; x = torch.randn((10, 10), device=DEVICE)\n&gt;&gt;&gt; y = model(x)\n&gt;&gt;&gt; y.device\ndevice(type='mps', index=0)\n\n\n\n\n\n\n\nUsing wandb:\n\n\nez.setup_wandb(project_name='ezpz')\n\n\n\n\nFull support for any {device + framework + backend}:\n\ndevice: {GPU, XPU, MPS, CPU}\nframework: {torch, deepspeed, horovod, tensorflow}\nbackend: {DDP, deepspeed, horovod}"
  },
  {
    "objectID": "slides.html#example",
    "href": "slides.html#example",
    "title": "ezpz ğŸ‹",
    "section": "ğŸ“ Example",
    "text": "ğŸ“ Example\nWe provide below a complete example that will launch test_dist.py (included below) across all GPUs in your current {PBS, slurm} job and train a simple model using either DDP or deepspeed\n\n\ntest_dist.py\n\n\n\"\"\"\nezpz_ddp.py\n\n- to launch:\n\n$ source ezpz/src/ezpz/bin/savejobenv\n$ BACKEND=DDP launch python3 ezpz_ddp.py\n\"\"\"\nimport os\nimport logging\nimport time\nfrom typing import Optional\nimport torch\nimport ezpz as ez\n\n# backend can be any of DDP, deespepeed, horovod\nRANK = ez.setup_torch(\n  backend=(\n      backend := os.environ.get('BACKEND', 'DDP')\n  ),\n  port=(\n      port := os.environ.get(\"MASTER_PORT\", \"29500\")\n  )\n)\n# RANK = DIST_INIT['rank']\n# WORLD_SIZE = DIST_INIT['world_size']\n# LOCAL_RANK = DIST_INIT['local_rank']\n# if DEVICE == \"cuda\" and torch.cuda.is_available():\n#     torch.cuda.set_device(LOCAL_RANK)\nDEVICE = ez.get_torch_device()\nWORLD_SIZE = ez.get_world_size()\nLOCAL_RANK = ez.get_local_rank()\nDEVICE_ID = f\"{DEVICE}:{LOCAL_RANK}\"\n\n\n# log only from RANK == 0\nlogger = logging.getLogger(__name__)\nlogger.setLevel(\"INFO\") if RANK == 0 else logger.setLevel(\"CRITICAL\")\n\nBATCH_SIZE = int(os.environ.get(\"BATCH_SIZE\", 64))  # 64\nINPUT_SIZE = int(os.environ.get(\"INPUT_SIZE\", 128))  # 128\nOUTPUT_SIZE = int(os.environ.get(\"OUTPUT_SIZE\", 128))  # 128\nDTYPE = os.environ.get(\"DTYPE\", torch.get_default_dtype())\nTRAIN_ITERS = int(os.environ.get(\"TRAIN_ITERS\", 50))\n\n# logger.info(f\"{DIST_INIT=}\")\n\n\nclass Network(torch.nn.Module):\n  def __init__(\n          self,\n          input_dim: int = 128,\n          output_dim: int = 128,\n          sizes: Optional[list[int]] = None,\n  ):\n      super(Network, self).__init__()\n      if sizes is None:\n          self.layers = torch.nn.Linear(input_dim, output_dim)\n      elif len(sizes) &gt; 0:\n          layers = [torch.nn.Linear(input_dim, sizes[0])]\n          for idx, size in enumerate(sizes[1:]):\n              layers.append(\n                  torch.nn.Linear(sizes[idx], size)\n              )\n          layers.append(torch.nn.Linear(sizes[-1], output_dim))\n          self.layers = torch.nn.Sequential(*layers)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n      return self.layers(x)\n\n\ndef calc_loss(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n  return (y - x).pow(2).sum()\n\n\ndef plot_losses(losses: dict) -&gt; None:\n  import plotext as pltx\n  # y = list(losses.values())\n  pltx.theme('clear')\n  pltx.scatter(list(losses.values()))\n  pltx.show()\n  pltx.save_fig(\"test_dist_losses.txt\")\n  pltx.ylabel(\"loss\")\n  pltx.xlabel(\"iteration\")\n\n\ndef main():\n  model = Network(\n      input_dim=INPUT_SIZE,\n      output_dim=OUTPUT_SIZE,\n      sizes=[1024, 512, 256, 128]\n  )\n  model.to(DEVICE)\n  model.to(DEVICE_ID)\n  logger.info(f'{model=}')\n  optimizer = torch.optim.Adam(model.parameters())\n  if backend.lower() == 'ddp':\n      if WORLD_SIZE &gt; 1:\n          from torch.nn.parallel import DistributedDataParallel as DDP\n          model = DDP(\n              model,\n              device_ids=[]\n          )\n  elif backend.lower() in ('ds', 'deepspeed'):\n      import deepspeed\n      # config = ez.load_ds_config().update(\n      #     {\"train_micro_batch_size_per_gpu\": BATCH_SIZE}\n      # )\n      import argparse\n      parser = argparse.ArgumentParser(\n          description='My training script.'\n      )\n      parser.add_argument(\n          '--local_rank',\n          required=False,\n          type=int,\n          default=-1,\n          # default=ez.get_local_rank()),\n          help='local rank passed from distributed launcher',\n      )\n      # Include DeepSpeed configuration arguments\n      parser = deepspeed.add_config_arguments(parser)\n      cmd_args = parser.parse_args()\n      logger.info(f'{cmd_args=}')\n      model, optimizer, *_ = deepspeed.initialize(\n          args=cmd_args,\n          model=model,\n          optimizer=optimizer,\n      )\n\n  losses = {}\n  for iter in range(TRAIN_ITERS):\n      t0 = time.perf_counter()\n      x = torch.rand((BATCH_SIZE, INPUT_SIZE), dtype=DTYPE).to(DEVICE)\n      y = model(x)\n      loss = calc_loss(x, y)\n      losses[iter] = loss\n      dtf = ((t1 := time.perf_counter()) - t0)\n      if backend == 'deepspeed':\n          model.backward(loss)\n          model.step(loss)\n      else:\n          loss.backward()\n          optimizer.step()\n      optimizer.zero_grad()\n      dtb = time.perf_counter() - t1\n      logger.info(\n          ', '.join([\n              f'{iter=}',\n              f'loss={loss.item():.5f}',\n              f'dt={dtf+dtb:.3f}',\n              f'{dtf=:.3f}',\n              f'{dtb=:.3f}'\n          ])\n      )\n  if RANK == 0:\n      plot_losses(losses)\n\n\nif __name__ == '__main__':\n  main()\n\n\nğŸƒğŸ»â€â™‚ï¸ Running\n\ngit clone + pip install ezpz:\n$ git clone https://github.com/saforem2/ezpz\n$ python3 -m pip install -e ezpz\n[optional] If using PBS or slurm:\n\n\n\nSave Job info:\n\n\nsavejobenv:\n$ source ezpz/src/ezpz/bin/savejobenv\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ Writing PBS vars to /home/foremans/.pbsenv\nâ”‚ HOSTFILE: /var/spool/pbs/aux/8992614.amn-0001\nâ”‚ NHOSTS: 2\nâ”‚ NGPU_PER_HOST: 12 GPUs per host\nâ”‚ NGPUS: 24 GPUs total\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [DIST INFO]:\nâ”‚   â€¢ Writing Job info to /home/foremans/.pbsenv\nâ”‚     â€¢ HOSTFILE: /var/spool/pbs/aux/8992614.amn-0001\nâ”‚     â€¢ NHOSTS: 2\nâ”‚     â€¢ NGPU_PER_HOST: 12\nâ”‚     â€¢ NGPUS = (NHOSTS * NGPU_PER_HOST) = 24\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Hosts]:\nâ”‚       â€¢ x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com, x1921c0s2b0n0.hostmgmt2000.cm.americas.sgi.com\nâ”‚     â€¢ [host:0] - x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\nâ”‚     â€¢ [host:1] - x1921c0s2b0n0.hostmgmt2000.cm.americas.sgi.com\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ YOU ARE HERE: /home/foremans\nâ”‚ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Launch]:\nâ”‚     â€¢ Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/8992614.amn-0001)\nâ”‚       to launch job\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nthis will automatically define a launch alias:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Launch]:\nâ”‚     â€¢ Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/8992614.amn-0001)\nâ”‚       to launch job\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\nLaunch test_dist.py:\n\nDDP:\n$ launch python3 -m ezpz.test_dist\nDeepSpeed:\n$ BACKEND=deepspeed launch python3 -m ezpz.test_dist --deepspeed --deepspeed_config ezpz/src/ezpz/conf/ds_config.json\nOutput:\n\n\n\nGPU\n\n$ launch python3 -m ezpz.test_dist |& tee ezpz-test-dist.log\n\nConnected to tcp://x3005c0s13b0n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /lus/eagle/projects/datascience/foremans/miniconda3/envs/2024-04-20/bin/python3\nLaunching application 9e4c8311-1729-4385-b1d2-d4cd6006ac1d\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=1/7][local_rank=1/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=5/7][local_rank=1/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=3/7][local_rank=3/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=7/7][local_rank=3/3][node=1/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=4/7][local_rank=0/3][node=0/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=6/7][local_rank=2/3][node=0/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=2/7][local_rank=2/3][node=0/1]\n[2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=0/7][local_rank=0/3][node=0/1]\n[2024-04-20 19:26:22][WARNING][dist:296] - Using [8 / 8] available \"cuda\" devices !!\n[2024-04-20 19:26:22][INFO][test_dist:46] - DIST_INIT={'world_size': 8, 'rank': 0, 'local_rank': 0}\n[2024-04-20 19:26:24][INFO][test_dist:84] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=0, loss=2789.99072, dt=0.664, dtf=0.659, dtb=0.005\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=1, loss=1961.33459, dt=0.002, dtf=0.001, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=2, loss=1450.47461, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=3, loss=1088.81958, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=4, loss=945.28839, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=5, loss=906.78857, dt=0.002, dtf=0.000, dtb=0.001\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=6, loss=789.18243, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=7, loss=751.63477, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=8, loss=735.62915, dt=0.002, dtf=0.000, dtb=0.002\n[2024-04-20 19:26:28][INFO][test_dist:126] - iter=9, loss=732.12775, dt=0.002, dtf=0.000, dtb=0.001\n\n\n\nXPU\n\n# [04:50:57 PM] [foremans@x1921c0s0b0n0] ~/q/llm.devkit/Megatron-DeepSpeed/dep/ezpz/s/ezpz ï˜ main q4-drop 32s\n$ launch python3 -Wignore test_dist.py\nConnected to tcp://x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com:7919\nFound executable /home/foremans/miniconda3/envs/q4-drop/bin/python3\nLaunching application 5bf3e9e8-89fb-412a-a49e-3c81601436b7\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=9/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=14/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=3/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=17/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=6/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=13/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=7/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=19/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=8/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=21/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=10/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=22/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=11/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=23/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=2/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=20/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=4/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=15/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=18/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=12/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=1/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=16/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=5/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"ccl\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/8992337.amn-0001\",\n    \"HOSTNAME\": \"x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\",\n    \"HOSTS\": \"['x1921c0s0b0n0', 'x1921c0s5b0n0']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"SunSpot\",\n    \"NGPUS\": 24,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 24,\n    \"WORLD_SIZE_TOTAL\": 24\n}\n[2024-04-19 16:51:06][INFO][dist:602] - Using oneccl_bindings from: /lus/gila/projects/Aurora_deployment/foremans/q4-drop_sunspot/llm.devkit/torch-ccl/oneccl_bindings_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:604] - Using ipex from: /home/foremans/miniconda3/envs/q4-drop/lib/python3.9/site-packages/intel_extension_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:605] - [0/24] Using device='xpu' with backend='DDP' + 'ccl' for distributed training.\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=0/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][WARNING][dist:296] - Using [24 / 24] available \"xpu\" devices !!\n2024:04:19-16:51:06:(16909) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n[2024-04-19 16:51:06][INFO][test_dist:71] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=0, loss=2709.53418, dt=1.380, dtf=0.950, dtb=0.430\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=1, loss=2058.49805, dt=0.133, dtf=0.002, dtb=0.131\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=2, loss=1507.91187, dt=0.004, dtf=0.001, dtb=0.004\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=3, loss=1181.78577, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=4, loss=949.43561, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=5, loss=848.14905, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=6, loss=788.76123, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=7, loss=753.59509, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=8, loss=750.62225, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=9, loss=740.23474, dt=0.004, dtf=0.001, dtb=0.003\nApplication 5bf3e9e8 resources: utime=621s stime=111s maxrss=1746816KB inblock=192 oublock=16 minflt=10719359 majflt=7493 nvcsw=169332 nivcsw=77546\n\n\n\n\n\nCPU\n\n$ TORCH_DEVICE=cpu mpirun -np 12 python3 test_dist.py\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=1/11][local_rank=1/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=3/11][local_rank=3/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=6/11][local_rank=6/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=5/11][local_rank=5/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=2/11][local_rank=2/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=10/11][local_rank=10/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=4/11][local_rank=4/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=7/11][local_rank=7/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=9/11][local_rank=9/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=11/11][local_rank=11/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=8/11][local_rank=8/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"cpu\",\n    \"DEVICE_ID\": \"cpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/Users/samforeman/projects/saforem2/ezpz/src/ezpz/hostfile\",\n    \"HOSTNAME\": \"Sams-MacBook-Pro.local\",\n    \"HOSTS\": \"['Sams-MacBook-Pro']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Sams-MacBook-Pro.local\",\n    \"NGPUS\": 12,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 1,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"LOCAL\",\n    \"WORLD_SIZE_IN_USE\": 12,\n    \"WORLD_SIZE_TOTAL\": 12\n}\n[2024-04-19 14:44:13][INFO][dist:605] - [0/12] Using device='cpu' with backend='DDP' + 'gloo' for distributed training.\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=0/11][local_rank=0/11][node=0/0]\n[2024-04-19 14:44:13][WARNING][dist:296] - Using [12 / 12] available \"cpu\" devices !!\n[2024-04-19 14:44:13][INFO][test_dist:72] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=0, loss=2801.62549, dt=0.389, dtf=0.042, dtb=0.348\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=1, loss=2092.84692, dt=0.051, dtf=0.010, dtb=0.041\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=2, loss=1482.45520, dt=0.037, dtf=0.004, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=3, loss=1174.38037, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=4, loss=938.39917, dt=0.032, dtf=0.003, dtb=0.030\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=5, loss=888.37390, dt=0.035, dtf=0.001, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=6, loss=784.63470, dt=0.036, dtf=0.003, dtb=0.032\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=7, loss=749.53839, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=8, loss=732.22656, dt=0.036, dtf=0.003, dtb=0.034\n[2024-04-19 14:44:15][INFO][test_dist:102] - iter=9, loss=730.63776, dt=0.034, dtf=0.001, dtb=0.033\n35.68s user 17.20s system 546% cpu 9.681s total"
  },
  {
    "objectID": "slides.html#helper-utilities",
    "href": "slides.html#helper-utilities",
    "title": "ezpz ğŸ‹",
    "section": "ğŸ§° Helper Utilities",
    "text": "ğŸ§° Helper Utilities\nWe provide some shell scripts that are useful when working with a job scheduler (e.g.Â PBS Pro @ ALCF or slurm elsewhere).\n\nsrc/ezpz/bin/savejobenv:\nShell script to save relevant job related environment variables to a file which can be sourced from new login instances.\n\n\n\nsavejobenv\n\n\nLaunch a job, clone (or navigate into) ezpz, and source src/ezpz/bin/savejobenv:\n\n(thetalogin4) $ qsub-gpu -A datascience -n 2 -q full-node --attrs=\"filesystems=home,grand,eagle,theta-fs0:ssds=required\" -t 06:00 -I\nJob routed to queue \"full-node\".\nWait for job 10155652 to start...\nOpening interactive session to thetagpu04\n[...]\n(thetagpu04) $ git clone https://github.com/saforem2/ezpz\n(thetagpu04) $ source ezpz/src/ezpz/bin/savejobenv\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ Writing COBALT vars to /home/foremans/.cobaltenv\nâ”‚ HOSTFILE: /var/tmp/cobalt.10155652\nâ”‚ NHOSTS: 2\nâ”‚ 8 GPUs per host\nâ”‚ 16 GPUs total\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [DIST INFO]:\nâ”‚   â€¢ Writing Job info to /home/foremans/.cobaltenv\nâ”‚     â€¢ HOSTFILE: /var/tmp/cobalt.10155652\nâ”‚     â€¢ NHOSTS: 2\nâ”‚     â€¢ NGPU_PER_HOST: 8\nâ”‚     â€¢ NGPUS = (NHOSTS * NGPU_PER_HOST) = 16\nâ”‚ [Hosts]:\nâ”‚       â€¢ thetagpu04 thetagpu19\nâ”‚ [Launch]:\nâ”‚     â€¢ Use: 'launch' (=mpirun -n  -N  --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH)\nâ”‚       to launch job\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ YOU ARE HERE: /home/foremans\nâ”‚ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\nsrc/ezpz/bin/getjobenv:\nShell script that, when sourced, will populate the current environment with the necessary job-related variables.\n\n\n\ngetjobenv\n\n\nNow, in a NEW SHELL\n(localhost)   $ ssh &lt;user&gt;@theta\n(thetalogin4) $ ssh thetagpu19\n(thetagpu19)  $ module load conda/2023-01-11; conda activate base\n(thetagpu19)  $ cd ezpz\n(thetagpu19)  $ source ./src/ezpz/bin/getjobenv\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [Hosts]: \nâ”‚     â€¢ thetagpu04, thetagpu19\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ [DIST INFO]: \nâ”‚     â€¢ Loading job env from: /home/foremans/.cobaltenv\nâ”‚     â€¢ HOSTFILE: /var/tmp/cobalt.10155652\nâ”‚     â€¢ NHOSTS: 2\nâ”‚     â€¢ NGPU_PER_HOST: 8\nâ”‚     â€¢ NGPUS (NHOSTS x NGPU_PER_HOST): 16\nâ”‚     â€¢ DIST_LAUNCH: mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\nâ”‚     â€¢ Defining alias: launch: aliased to mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n(thetagpu19) $ mkdir -p venvs/thetaGPU/2023-01-11\n(thetagpu19) $ python3 -m venv venvs/thetaGPU/2023-01-11 --system-site-packages\n(thetagpu19) $ source venvs/thetaGPU/2023-01-11/bin/activate\n(thetagpu19) $ python3 -m pip install -e . --require-virtualenv\n(thetagpu19) $ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-10-26 12:21:26,716][ezpz.dist][INFO] - Using DDP for distributed training\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 13\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 14\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 8\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 12\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 9\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 10\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 15\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 11\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2\n[2023-10-26 12:21:26,798][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n[2023-10-26 12:21:26,811][torch.distributed.distributed_c10d][INFO] - Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,812][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,814][torch.distributed.distributed_c10d][INFO] - Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,815][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,816][torch.distributed.distributed_c10d][INFO] - Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,817][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,819][torch.distributed.distributed_c10d][INFO] - Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,820][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,821][torch.distributed.distributed_c10d][INFO] - Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,823][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,827][torch.distributed.distributed_c10d][INFO] - Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,828][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,830][torch.distributed.distributed_c10d][INFO] - Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,831][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:27,035][ezpz.dist][INFO] - RANK: 0 / 15\n{\n  \"framework\": \"pytorch\",\n  \"backend\": \"DDP\",\n  \"use_wandb\": false,\n  \"seed\": null,\n  \"port\": null,\n  \"ds_config_path\": null,\n  \"wandb_project_name\": null,\n  \"precision\": null,\n  \"ngpus\": null\n}\n[2023-10-26 12:21:27,038][__main__][INFO] - Output dir: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/ezpz/outputs/runs/pytorch/DDP/2023-10-26/12-21-25\n[2023-10-26 12:21:27,097][ezpz.dist][INFO] - RANK: 8 / 15\n[2023-10-26 12:21:27,103][ezpz.dist][INFO] - RANK: 6 / 15\n[2023-10-26 12:21:27,104][ezpz.dist][INFO] - RANK: 14 / 15\n[2023-10-26 12:21:27,111][ezpz.dist][INFO] - RANK: 13 / 15\n[2023-10-26 12:21:27,116][ezpz.dist][INFO] - RANK: 1 / 15\n[2023-10-26 12:21:27,126][ezpz.dist][INFO] - RANK: 7 / 15\n[2023-10-26 12:21:27,135][ezpz.dist][INFO] - RANK: 10 / 15\n[2023-10-26 12:21:27,139][ezpz.dist][INFO] - RANK: 12 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 9 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 15 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 11 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 5 / 15\n[2023-10-26 12:21:27,144][ezpz.dist][INFO] - RANK: 2 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 4 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 3 / 15\n16.56s user 30.05s system 706% cpu 6.595s total\nwhile this example looked at ThetaGPU, the exact same process will work on any of {ThetaGPU, Polaris, Perlmutter}.\n\n\n\n\n\n\n\n\nâ¤ï¸â€ğŸ©¹ Status\n\n\n\n\nLast Updated: 05/13/2024 @ 19:16:46"
  }
]